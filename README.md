# üß† Deep Q-Network for Atari Ms. Pac-Man

**Author:** Sri Lakshmi Swetha Jalluri  
**License:** MIT

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## üìã Table of Contents

- [Overview](#-overview)
- [Environment Analysis](#-environment-analysis)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Training](#-training)
- [Experimental Results](#-experimental-results)
- [Agent Gameplay](#-agent-gameplay)
- [Conceptual Analysis](#-conceptual-analysis)
- [Code Attribution](#-code-attribution)
- [Video Demonstration](#-video-demonstration)
- [License](#-license)
- [References](#-references)

---

<img width="160" height="210" alt="image" src="https://github.com/user-attachments/assets/e4e1059f-417b-4977-a680-010b57e14571" />

<img width="160" height="210" alt="image" src="https://github.com/user-attachments/assets/3008acfc-ff1b-43fa-af55-59e15ed7caff" />

<img width="160" height="210" alt="image" src="https://github.com/user-attachments/assets/2044983a-d1f4-429c-92fc-bd768fdca5ce" />

<img width="160" height="210" alt="image" src="https://github.com/user-attachments/assets/ddf60633-b888-437a-bc3b-961cd6f85710" />

---

## üéØ Overview

This project implements a **Deep Q-Network (DQN)** agent that learns to play Atari Ms. Pac-Man through reinforcement learning. The agent processes raw pixel observations and learns optimal policies to maximize game score through trial-and-error interaction with the environment.

**Key Features:**
- ‚úÖ Experience replay for sample efficiency
- ‚úÖ Target network for training stability
- ‚úÖ Frame stacking for temporal information
- ‚úÖ Multiple exploration strategies (Œµ-greedy, softmax)
- ‚úÖ Comprehensive hyperparameter experiments
- ‚úÖ Detailed performance analysis and visualization

**Best Performance:** Average return of **39.4 ¬± 10.4** using optimized learning rate (0.0005)

---

## üïπÔ∏è Environment Analysis

### **ALE/MsPacman-v5 Specifications**

| Property | Description |
|----------|-------------|
| **Environment ID** | `ALE/MsPacman-v5` |
| **Observation Space** | `Box(0, 255, (210, 160, 3), uint8)` ‚Üí Preprocessed to `(4, 84, 84)` |
| **Action Space** | `Discrete(9)` - 8 directional moves + NOOP |
| **State Representation** | 4 stacked grayscale frames (84√ó84 each) |
| **Reward Type** | Integer game score increments |

### **Action Space Breakdown**

| Action ID | Direction | Description |
|-----------|-----------|-------------|
| 0 | NOOP | No operation / stay still |
| 1 | UP | Move upward |
| 2 | RIGHT | Move right |
| 3 | LEFT | Move left |
| 4 | DOWN | Move downward |
| 5 | UPRIGHT | Diagonal up-right |
| 6 | UPLEFT | Diagonal up-left |
| 7 | DOWNRIGHT | Diagonal down-right |
| 8 | DOWNLEFT | Diagonal down-left |

### **Reward Events**

| Event | Reward | Strategic Value |
|-------|--------|-----------------|
| Eating normal pellet | +10 | Primary objective (maze clearance) |
| Eating power pellet | +50 | Enables ghost-hunting opportunities |
| Eating ghost (1st) | +200 | High-value risky behavior |
| Eating ghost (2nd consecutive) | +400 | Escalating rewards for strategy |
| Eating ghost (3rd consecutive) | +800 | Maximum strategic execution |
| Eating ghost (4th consecutive) | +1600 | Perfect sequence completion |
| Eating bonus fruit | +100-5000 | Bonus objectives |
| Death | Episode termination | Implicit negative reward |

**Why This Reward Structure Works:**
- **Aligned with objectives:** Maximizing score = playing well
- **Sparse but meaningful:** Clear signals without noise
- **Natural curriculum:** Pellets ‚Üí Power pellets ‚Üí Ghost hunting
- **Risk-reward balance:** High ghost rewards balanced by death risk

### **State Space Analysis**

**Raw State:** 210 √ó 160 √ó 3 RGB image = 100,800 dimensions

**Preprocessed State:**
1. Convert RGB ‚Üí Grayscale: 210 √ó 160 √ó 1
2. Resize to 84 √ó 84: Reduces computation
3. Stack 4 frames: Captures motion (current + 3 previous)
4. Final shape: 4 √ó 84 √ó 84 = 28,224 dimensions

**Why Stack Frames?**
- Single frames cannot capture motion direction or velocity
- 4 frames allow agent to infer: "Ghost is moving left" or "Ms. Pac-Man is approaching pellet"
- Critical for temporal decision-making in dynamic environments

**Q-Table Size (Why DQN is Necessary):**
- Possible states: 256^(4√ó84√ó84) ‚âà 10^68,000 (impossible to enumerate!)
- With 9 actions: Q-table would need 9 √ó 10^68,000 entries
- **Solution:** Neural network approximates Q(s,a) with ~1.5M parameters

---

## üèóÔ∏è Architecture

### **DQN Network Structure**

```python
Input: (batch, 4, 84, 84) - 4 stacked grayscale frames

Conv1: 4 ‚Üí 32 channels, 8√ó8 kernel, stride 4, ReLU
       Output: (batch, 32, 20, 20)

Conv2: 32 ‚Üí 64 channels, 4√ó4 kernel, stride 2, ReLU
       Output: (batch, 64, 9, 9)

Conv3: 64 ‚Üí 64 channels, 3√ó3 kernel, stride 1, ReLU
       Output: (batch, 64, 7, 7)

Flatten: 7√ó7√ó64 = 3,136 features

FC1: 3,136 ‚Üí 512, ReLU
FC2: 512 ‚Üí 9 (Q-values for each action)

Initialization: Kaiming uniform (optimized for ReLU)
Normalization: Input pixels / 255.0
```

**Architecture Decisions:**
- **3 Convolutional Layers:** Progressively extract hierarchical features (edges ‚Üí objects ‚Üí spatial relationships)
- **Aggressive Downsampling:** Reduces 84√ó84 ‚Üí 7√ó7 for computational efficiency
- **ReLU Activations:** Standard for deep RL, prevents vanishing gradients
- **Kaiming Initialization:** Optimized for ReLU networks, improves convergence

### **Key DQN Components**

**1. Experience Replay Buffer**
```python
Capacity: 100,000 transitions
Storage: (state, action, reward, next_state, done)
Sampling: Uniform random batches of 32
Data type: uint8 for memory efficiency
```

**Benefits:**
- Breaks temporal correlations between consecutive experiences
- Enables multiple updates from single experience
- Improves sample efficiency

**2. Target Network**
```python
Update frequency: Every 10,000 frames
Purpose: Provides stable Q-value targets
Mechanism: Periodic copy of online network weights
```

**Why Needed:**
- Prevents "chasing moving target" problem
- Reduces oscillations in Q-value estimates
- Dramatically improves training stability

**3. Training Procedure**
```python
Batch size: 32
Training frequency: Every 4 frames
Learning starts: After 5,000 frames (warm-up period)
Optimizer: Adam (lr = 0.00025 baseline, 0.0005 best)
Loss function: Smooth L1 Loss (Huber Loss)
Gradient clipping: max_norm = 10.0
```

---

## üíª Installation

### **Requirements**

```bash
# Python 3.9 or higher required
python --version
```

### **Setup Instructions**

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/dqn-mspacman.git
cd dqn-mspacman

# 2. Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Accept Atari ROMs license (required)
pip install gymnasium[accept-rom-license]
```

### **Dependencies** (`requirements.txt`)

```
gymnasium==0.28.1
ale-py==0.8.1
shimmy==0.2.1
torch==2.4.0
numpy==1.26.4
opencv-python==4.10.0.84
matplotlib==3.8.4
tqdm==4.66.4
pyyaml==6.0.2
autorom==0.6.1
AutoROM.accept-rom-license==0.6.1
```

### **Verify Installation**

```bash
# Test environment loading
python src/smoke_test.py

# Expected output:
# Env OK: Discrete(9) (210, 160, 3)
# Random run reward (10 steps): [some number]
```

---

## üöÄ Training

### **Quick Start - Baseline Training**

```bash
# Train baseline DQN agent (500k frames, ~8 hours on Apple Silicon)
python src/train.py --config configs/baseline.yaml

# Training will save:
# - Model checkpoints: runs/baseline-egreedy-[timestamp]/best.pt
# - Training logs: runs/baseline-egreedy-[timestamp]/train_log.csv
# - Configuration: runs/baseline-egreedy-[timestamp]/config.json
```

### **Run Specific Experiments**

```bash
# Experiment 1: Faster epsilon decay
python src/train.py --config configs/epsfast.yaml

# Experiment 2: Lower discount factor
python src/train.py --config configs/gamma95.yaml

# Experiment 3: Higher learning rate (best performer)
python src/train.py --config configs/lr0005.yaml

# Experiment 4: Softmax exploration
python src/train.py --config configs/softmax.yaml
```

### **Configuration Parameters**

Example `baseline.yaml`:
```yaml
env_id: "ALE/MsPacman-v5"
seed: 42
device: "mps"  # Use "cuda" for NVIDIA GPU, "cpu" for CPU

# Preprocessing
frame_stack: 4
resize: [84, 84]
gray_scale: true
clip_rewards: true

# DQN Core
gamma: 0.99
learning_rate: 0.00025
batch_size: 32
buffer_size: 100000
target_update_freq: 10000

# Exploration
exploration: "epsilon_greedy"
epsilon_start: 1.0
epsilon_end: 0.05
epsilon_decay_frames: 1000000

# Training
total_frames: 500000
eval_episodes: 10
```

### **Evaluation**

```bash
# Evaluate trained model (10 episodes)
python src/eval_agent.py --run_dir runs/lr0005-300k-[timestamp] --episodes 10

# With JSON output
python src/eval_agent.py --run_dir runs/lr0005-300k-[timestamp] --episodes 10 --save_json
```

### **Visualization**

```bash
# Generate training plots
python src/plot_training.py runs/lr0005-300k-[timestamp]

# Outputs:
# - plot_returns.png - Episode returns over time
# - plot_lengths.png - Episode lengths over time
# - plot_loss.png - Training loss curve
```

---

## üìä Experimental Results

### **Configurations Tested**

| Configuration | Learning Rate | Gamma | Epsilon Decay | Exploration | Frames |
|--------------|---------------|-------|---------------|-------------|--------|
| baseline-egreedy | 0.00025 | 0.99 | 1M frames | Œµ-greedy | 500k |
| epsfast-300k | 0.00025 | 0.99 | 500k frames | Œµ-greedy | 300k |
| epsfloor-300k | 0.00025 | 0.99 | 500k frames | Œµ-greedy | 300k |
| gamma95-300k | 0.00025 | **0.95** | 1M frames | Œµ-greedy | 300k |
| **lr0005-300k** | **0.0005** | 0.99 | 1M frames | Œµ-greedy | 300k |
| softmax-300k | 0.00025 | 0.99 | 1M frames | **Softmax** | 300k |

### **Performance Summary**

| Run | Eval Return | Eval Steps | Train Episodes | Loss (final) |
|-----|-------------|------------|----------------|--------------|
| baseline-egreedy | 19.0 ¬± 4.0 | **646.8** ¬± 54.6 | 917 | 0.0672 |
| epsfast-300k | 26.9 ¬± 7.38 | 552.6 ¬± 69.1 | 557 | 0.0418 |
| epsfloor-300k | 26.9 ¬± 7.38 | 552.6 ¬± 69.1 | 557 | 0.0418 |
| gamma95-300k | 20.3 ¬± 0.46 | **770.0** ¬± 111.5 | 574 | 0.0286 |
| **lr0005-300k** ‚≠ê | **39.4** ¬± 10.4 | 541.0 ¬± 104.8 | 574 | 0.0971 |
| softmax-300k | 30.1 ¬± 8.65 | 512.8 ¬± 84.9 | 562 | 0.0988 |

### **Key Findings**

**üèÜ Best Model: lr0005-300k**
- **+107% improvement** over baseline (39.4 vs 19.0 average return)
- Doubled learning rate (0.0005 vs 0.00025) enabled faster convergence
- Achieved stable performance within 300k frames
- Training episodes reached peaks of 60-85 points

**üìà Learning Rate Impact**
- Baseline (lr=0.00025): 19.0 average return
- Doubled (lr=0.0005): 39.4 average return
- **Insight:** Conservative learning rate was bottleneck for 300k frame budget

**üé≤ Discount Factor Analysis**
- Baseline (Œ≥=0.99): 19.0 return, 646.8 steps
- Reduced (Œ≥=0.95): 20.3 return, **770.0 steps**
- **Paradox:** Lower Œ≥ ‚Üí longer survival but lower scores
- **Interpretation:** More defensive play (avoids risky ghost-chasing)

**üîç Exploration Strategy Comparison**
- Œµ-greedy baseline: 19.0 return
- Œµ-greedy fast decay: 26.9 return (+41%)
- Softmax: 30.1 return (+58%)
- **Insight:** Softmax's informed exploration outperforms uniform random

**‚ö†Ô∏è Experimental Note:**
- `epsfast-300k` and `epsfloor-300k` produced identical results
- Both configs have same epsilon decay schedule (500k frames)
- Likely duplicate run or config error

### **Training Curves**

**Episode Return vs Frames**
![Episode Returns](docs/images/baseline_returns.png)
*Training shows gradual improvement from 15-25 average to 40-85 peaks*

**Episode Length vs Frames**
![Episode Lengths](docs/images/baseline_lengths.png)
*Episode duration stabilizes around 400-700 steps, indicating consistent gameplay*

**Training Loss vs Frames**
![Training Loss](docs/images/baseline_loss.png)
*Loss increases from 0.01 ‚Üí 0.20+ over training (expected in DQN as Q-values grow)*

### **Performance Metrics**

**Average Steps Per Episode:**

| Metric | Baseline | Epsfast | Gamma95 | Lr0005 | Softmax |
|--------|----------|---------|---------|--------|---------|
| Eval Steps | 646.8 | 552.6 | **770.0** | 541.0 | 512.8 |
| Train Episodes | 917 | 557 | 574 | 574 | 562 |
| Best Episode Return | ~90 | ~84 | ~79 | ~90 | ~85 |

**Analysis:**
- Longer evaluation episodes indicate better survival skills
- Gamma95 achieved longest episodes (770) through defensive play
- Lr0005 balanced survival (541 steps) with aggressive scoring (39.4 avg)

**Bellman Equation Parameters:**

**Baseline Choice Rationale:**
- **Œ± = 0.00025:** Small learning rate for neural network stability, prevents large Q-value oscillations
- **Œ≥ = 0.99:** Far-sighted planning horizon
  - Effective horizon: ~460 steps (where 0.99^460 ‚âà 0.01)
  - Enables strategic sequences: navigate ‚Üí power pellet ‚Üí hunt ghosts
  - Reward 100 steps away retains 37% of immediate value

**Alternative Parameter Tests:**

**Œ≥ = 0.95 Experiment:**
```
Effective horizon: ~90 steps (vs 460 for Œ≥=0.99)
Result: More myopic agent
Performance: 20.3 return (slightly better than baseline 19.0)
Episode length: 770 steps (much longer - more defensive)
Interpretation: Agent plays cautiously, avoids risky ghost-chasing
```

**lr = 0.0005 Experiment:**
```
2√ó baseline learning rate
Result: Much more aggressive learning
Performance: 39.4 return (107% better than baseline!)
Interpretation: Faster convergence to effective policies within 300k budget
Trade-off: Higher variance (¬±10.4 vs ¬±4.0) due to aggressive updates
```

---

## üéÆ Agent Gameplay

### **Trained Agent Performance**

The agent demonstrates learned behaviors including:
- ‚úÖ Strategic pellet collection (systematic maze clearing)
- ‚úÖ Ghost avoidance patterns (maintains safe distance)
- ‚úÖ Power pellet utilization (occasional ghost-hunting sequences)
- ‚úÖ Spatial navigation (efficient pathfinding through maze)

### **Gameplay Screenshots**

| Frame 50 (Score: 50) | Frame 150 (Score: 150) |
|:----:|:----:|
| ![Frame 50](docs/images/gameplay_frame_50.png) | ![Frame 150](docs/images/gameplay_frame_150.png) |
| Early gameplay - learning pellet collection | Mid-game - avoiding ghosts while clearing maze |

| Frame 300 (Score: 300) | Frame 420 (Score: 420) |
|:----:|:----:|
| ![Frame 300](docs/images/gameplay_frame_300.png) | ![Frame 420](docs/images/gameplay_frame_420.png) |
| Strategic positioning near power pellet | Advanced play - efficient navigation |

**Observable Behaviors:**
1. **Early Game (0-100 frames):** Random exploration, inefficient movement
2. **Mid Game (100-200 frames):** Learned pellet targeting, basic ghost avoidance
3. **Late Game (200-400 frames):** Strategic positioning, efficient maze clearing
4. **Peak Performance:** Occasional power pellet ‚Üí ghost-chasing sequences (200-1600 point combos)

---

## üß† Conceptual Analysis

### **1. Q-Learning Classification: Value-Based vs Policy-Based**

**Q-Learning is definitively a value-based method.**

**Detailed Explanation:**

Q-learning learns a **value function** Q(s,a) that estimates expected cumulative reward for each state-action pair. The policy is **derived implicitly** from these values:

```
œÄ(s) = argmax_a Q(s,a)
```

**Value-Based Characteristics:**
- Learns Q(s,a) directly through Bellman updates
- Policy is deterministic greedy selection
- Updates based on TD error: Œ¥ = [R + Œ≥ max Q(s',a')] - Q(s,a)
- Off-policy: Can learn optimal policy while following exploratory behavior

**Contrast with Policy-Based Methods:**

Policy-based approaches (REINFORCE, PPO, A3C) directly parameterize the policy œÄ(a|s; Œ∏) and optimize it using policy gradients:

```
‚àáJ(Œ∏) = E[‚àá log œÄ(a|s; Œ∏) √ó Q(s,a)]
```

**Key Differences:**

| Aspect | Value-Based (Q-Learning) | Policy-Based (REINFORCE) |
|--------|--------------------------|--------------------------|
| **What's Learned** | Q-values Q(s,a) | Policy œÄ(a\|s) directly |
| **Policy Type** | Deterministic (argmax) | Can be stochastic |
| **Update Rule** | Bellman equation | Policy gradient |
| **Off-Policy** | Yes (learn from any data) | Usually no (on-policy) |
| **Continuous Actions** | Difficult (need argmax over continuous space) | Natural (output action directly) |

**In Ms. Pac-Man:**

Our DQN learns Q-values for all 9 actions and selects the highest:
```python
q_values = network(state)  # [Q(s,UP), Q(s,RIGHT), Q(s,LEFT), ...]
action = argmax(q_values)  # Pick action with highest value
```

We never explicitly represent œÄ(a|s)‚Äîit's always implicitly "pick the best Q-value."

**Why Value-Based Works Here:**
- Discrete action space (9 actions) makes argmax tractable
- Deterministic policies are effective for arcade games
- Off-policy learning enables experience replay
- Q-values provide interpretable measure of action quality

---

### **2. Expected Lifetime Value in Bellman Equation**

**What does "expected lifetime value" mean?**

The expected lifetime value V(s) or Q(s,a) represents the **total discounted reward** an agent anticipates accumulating from a state (or state-action pair) until the episode ends, averaged over all possible futures.

**Mathematical Definition:**
```
Q(s,a) = E[R_t + Œ≥R_{t+1} + Œ≥¬≤R_{t+2} + Œ≥¬≥R_{t+3} + ... | S_t=s, A_t=a]
       = E[Œ£_{k=0}^‚àû Œ≥^k R_{t+k} | S_t=s, A_t=a]
```

**Breaking Down Each Term:**

**"Expected" (E[¬∑]):**
- Averages over stochastic outcomes
- In Ms. Pac-Man: Ghost movements have some randomness, our Œµ-greedy policy is stochastic
- Must consider all possible trajectories and their probabilities

**"Lifetime":**
- The complete future from current timestep to episode termination
- Includes all pellets eaten, ghosts captured, fruits collected until death
- Not just next step‚Äîthe entire remaining episode

**"Value":**
- A scalar metric summarizing total future reward
- Allows comparing different actions/states numerically
- Higher value = better expected outcome

**The Discount Factor Œ≥'s Role:**

With Œ≥=0.99, future rewards are weighted:
```
Immediate reward (t+0):   1.000 √ó R  (100% value)
Reward at t+10:          0.904 √ó R  (90% value)
Reward at t+50:          0.605 √ó R  (60% value)
Reward at t+100:         0.366 √ó R  (37% value)
Reward at t+200:         0.134 √ó R  (13% value)
```

**Concrete Ms. Pac-Man Example:**

**Situation:** Ms. Pac-Man at junction, two path choices:

**Path A: Move RIGHT (toward nearby pellet)**
- Step 1: +10 (pellet immediately)
- Steps 2-5: +0 (dead-end corridor)
- Expected lifetime value: 10 + 0.99√ó0 ‚âà **10 points**

**Path B: Move UP (toward power pellet corridor)**
- Steps 1-20: +0 (navigating to power pellet)
- Step 21: +50 (power pellet)
- Steps 22-30: +200, +400, +800 (eating 3 vulnerable ghosts)
- Steps 31-60: +10√ó30 (30 pellets in cleared area)
- Expected discounted value:
```
Q(s,UP) ‚âà 0.99^21√ó50 + 0.99^25√ó200 + 0.99^28√ó400 + 0.99^31√ó800 + 0.99^40√ó300
        ‚âà 0.81√ó50 + 0.78√ó200 + 0.76√ó400 + 0.73√ó800 + 0.67√ó300
        ‚âà 40 + 156 + 304 + 584 + 201
        ‚âà 1,285 points
```

The "lifetime value" of Path B is ~128√ó higher than Path A, even though Path A gives immediate reward!

**Why This Matters:**

Without considering lifetime value, the agent would greedily grab nearby pellets. With lifetime value and Œ≥=0.99, the agent learns to:
- Forgo immediate small rewards for strategic positioning
- Execute multi-step plans (navigate ‚Üí power pellet ‚Üí ghost hunting)
- Balance short-term safety with long-term opportunity

**The Recursive Bellman Structure:**

Instead of summing infinite future rewards, we can express lifetime value recursively:
```
Q(s,a) = E[R + Œ≥ √ó V(s')]
       = E[R + Œ≥ √ó max_{a'} Q(s',a')]
```

"The value of taking action a in state s equals immediate reward R plus discounted value of the best next action."

This recursive decomposition:
- Turns infinite sum into one-step problem
- Enables dynamic programming (iterative improvement)
- Allows bootstrapping (use current estimates to improve estimates)

**In Our DQN Training:**

Every gradient descent step implements this:
```python
target = reward + gamma * max(Q_target(next_state, actions))
loss = (Q_online(state, action) - target)¬≤
```

We're teaching the network: "Your Q-value should equal immediate reward plus discounted future value."

---

### **3. Q-Learning vs LLM-Based Agents**

**How does Deep Q-Learning differ from agents using Large Language Models?**

While both are sequential decision-making systems, DQN and LLM agents represent fundamentally different paradigms of intelligence.

**Learning Paradigm:**

| Aspect | DQN Agent | LLM Agent |
|--------|-----------|-----------|
| **Training Method** | Trial-and-error in environment | Pre-training on massive text + RLHF fine-tuning |
| **Learning Signal** | Numeric rewards (game score) | Human preferences / next-token prediction |
| **Sample Requirement** | Millions of frames | Pre-trained knowledge + thousands of human ratings |
| **Adaptation** | Gradual improvement over episodes | Few-shot or zero-shot task switching |

**State Representation:**

**DQN:**
```python
state = [4√ó84√ó84 numpy array]
# Raw pixels
# No semantic understanding
# Pattern recognition: "These pixel patterns correlate with high rewards"
```

**LLM:**
```python
state = "Ms. Pac-Man at position (42, 78). Red ghost at (40, 80) 
         approaching from right. Power pellet at (35, 85) available.
         Current score: 450."
# Symbolic, semantic representation
# Rich linguistic context
# Causal understanding: "Ghost approaching = danger"
```

**Action Selection:**

**DQN:**
```python
q_values = network(pixels)  # [0.5, 2.3, 1.1, -0.3, ...]
action = argmax(q_values)   # Pick highest (UP)
# Pure mathematical optimization
# No explicit reasoning
# ~1ms decision time
```

**LLM:**
```python
prompt = "Given game state, what should Ms. Pac-Man do?"
response = llm.generate(prompt)
# "I should move LEFT to avoid the approaching red ghost.
#  After reaching safety, I'll navigate toward the power pellet
#  to enable ghost-hunting opportunities."
action = parse_action(response)
# Explicit natural language reasoning
# ~1-2 second decision time
```

**Generalization:**

**DQN:**
- **Specialist:** Trained only for Ms. Pac-Man
- **No transfer:** Cannot play Breakout, Pong, or other games
- **Catastrophic forgetting:** Learning new task destroys old knowledge
- **Narrow competence:** Excellent at one specific task

**LLM:**
- **Generalist:** Same model handles games, coding, writing, math
- **Zero-shot transfer:** Can attempt new games without retraining
- **Persistent knowledge:** Retains capabilities across tasks
- **Broad competence:** Adequate at many diverse tasks

**Sample Efficiency:**

**DQN:**
- Requires **300,000-500,000 frames** for basic competence
- Must directly experience states to learn about them
- No prior knowledge‚Äîlearns everything from scratch
- Data-hungry: ~574 episodes √ó 500 steps = 287,000 experiences

**LLM:**
- Can work from **textual descriptions** alone
- Can learn game rules by reading without playing
- Leverages pre-trained common sense and world knowledge
- Few-shot: Adapts with 1-10 examples

**Interpretability:**

**DQN:**
```
Q: Why did you move left?
A: "Q(state, LEFT) = 2.47 was highest"
Q: Why was it highest?
A: [Cannot explain‚Äîpattern learned from thousands of similar situations]
```

**LLM:**
```
Q: Why did you move left?
A: "I moved left because:
    1. Red ghost approaching from right (3 squares away)
    2. Left corridor leads to power pellet
    3. After eating power pellet, I can chase the ghost
    4. Moving right would risk death
    Expected value: LEFT > RIGHT by ~150 points"
```

**Computational Trade-offs:**

| Aspect | DQN | LLM |
|--------|-----|-----|
| **Training Time** | Days-weeks on GPU | Already trained (use pre-trained) |
| **Inference Speed** | ~1-5ms per action | ~1-2 seconds per response |
| **Memory** | ~100MB replay buffer | ~10-100GB model parameters |
| **Continuous Learning** | Improves through play | Static without fine-tuning |

**Real-World Example: Autonomous Driving**

**DQN Approach:**
- Input: Camera images + LiDAR + speed
- Output: Steering angle, acceleration, brake pressure
- Training: Millions of miles of simulated driving
- Inference: Real-time control (milliseconds)
- Strength: Precise motor control, fast reactions
- Weakness: Cannot handle novel scenarios, no common sense

**LLM Approach:**
- Input: "Traffic light ahead turning yellow, car behind following close, wet road"
- Output: "I should begin gentle braking now because hard braking risks rear-end collision"
- Training: Pre-trained on driving manuals, safety regulations, physics
- Inference: Slower but more interpretable
- Strength: Common sense reasoning, handles novelty
- Weakness: Cannot provide precise continuous control

**The Hybrid Future:**

Most effective systems combine both:
```python
# LLM: Strategic planning
strategy = llm.plan("Navigate to grocery store, avoid highway construction")
# "Route: Take Main St ‚Üí Oak Ave ‚Üí Grocery. ETA 12 minutes."

# DQN: Tactical execution  
while not arrived:
    action = dqn.control(camera_feed, strategy)
    # Precise steering, speed control, obstacle avoidance
```

**Summary:**

DQN and LLMs represent complementary approaches:
- **DQN:** Optimized control through experience, fast, precise, narrow
- **LLM:** Flexible reasoning through language, slow, interpretable, broad
- **Future:** Hybrid systems leveraging both strengths

---

### **4. Reinforcement Learning for LLM Agents**

**How do RL concepts from this assignment apply to LLM training?**

Modern LLMs like ChatGPT, Claude, and Gemini use Reinforcement Learning from Human Feedback (RLHF) to align with human preferences. The RL concepts from our Ms. Pac-Man DQN directly transfer to LLM training.

**RLHF Pipeline:**

**Stage 1: Supervised Pre-training**
- LLM learns from massive text corpus (books, websites, code)
- Objective: Predict next token P(word_t | word_1, ..., word_{t-1})
- Result: Model learns language structure, facts, reasoning patterns
- Analogous to: Pre-training DQN vision network on ImageNet

**Stage 2: Reward Model Training**
- Collect human preference data: "Response A > Response B"
- Train reward model: R(prompt, response) ‚Üí score
- Objective: Learn to predict human preferences
- Analogous to: Designing reward function for Ms. Pac-Man

**Stage 3: RL Fine-tuning (PPO)**
- Use policy gradient method (typically Proximal Policy Optimization)
- Objective: Maximize E[reward_model(prompt, response)]
- This is where core RL concepts apply!

**RL Concepts in RLHF:**

**1. Exploration vs Exploitation**

**DQN (Œµ-greedy):**
```python
if random() < epsilon:
    action = random()      # Explore: try random actions
else:
    action = best_action() # Exploit: use learned policy
```

**LLM (Temperature):**
```python
if temperature_high:
    token = sample(softmax(logits / high_temp))  # Explore: diverse outputs
else:
    token = sample(softmax(logits / low_temp))   # Exploit: confident outputs
```

**Applications:**
- **Creative writing:** High temp (1.0) ‚Üí diverse, creative responses
- **Code generation:** Low temp (0.1) ‚Üí deterministic, correct code
- **General chat:** Medium temp (0.7) ‚Üí balanced variety and coherence

**2. Temporal Credit Assignment**

**DQN Challenge:**
- Action at t=0 (move toward power pellet) leads to reward at t=100 (+200 ghost)
- How do we credit the initial decision for the delayed reward?
- **Solution:** Bellman equation with Œ≥=0.99 propagates future rewards backward

**LLM Challenge:**
- Token at position 5 ("First, let's") affects response quality at position 500 (complete answer)
- How do we credit early tokens for final outcome?
- **Solution:** Policy gradients with advantage estimation A(s,a) = Q(s,a) - V(s)

**Both use same principle:** Discount and propagate future outcomes to earlier decisions.

**3. Value Functions**

**DQN:** Q(s,a) = expected future game score from state s taking action a

**LLM:** V(prompt, partial_response) = expected human rating for completing this response

**Both** learn to predict long-term outcomes from current states.

**4. Policy Optimization**

**DQN (Value-Based ‚Üí Implicit Policy):**
```
Learn Q(s,a) via TD learning
Derive policy: œÄ(s) = argmax_a Q(s,a)
```

**LLM (Policy-Based ‚Üí Direct Optimization):**
```
Learn policy œÄ(token|context) directly via PPO
Optimize: E_{responses~œÄ}[reward_model(response)]
```

Different methods, same goal: Find policy that maximizes expected rewards.

**5. Reward Shaping**

**DQN:**
- Base rewards: +10 pellets, +200 ghosts
- Could add shaped rewards: +0.1 for approaching pellets
- Risk: Reward hacking (agent exploits shaped rewards)

**LLM:**
- Base reward: Human preference comparisons
- Shaped rewards: Length penalties, style bonuses, safety constraints
- Risk: Reward hacking (verbose responses to game length rewards)

**6. Experience Replay Concept**

**DQN:**
```python
replay_buffer = deque(maxlen=100000)
# Store: (state, action, reward, next_state, done)
# Sample random batches for training
# Benefits: Breaks correlation, reuses data
```

**LLM:**
```python
preference_dataset = [(prompt, good_response, bad_response), ...]
# Sample random batches for reward model training
# Benefits: Diverse training data, prevents overfitting
```

**7. Real-World RLHF Example**

**Task:** Train LLM to write helpful code explanations

**Step 1: Supervised Learning**
```
Model learns basic code understanding from GitHub, Stack Overflow
Can explain code but quality varies
```

**Step 2: Collect Human Preferences**
```
Prompt: "Explain binary search"
Response A: "Binary search divides array in half repeatedly..."
Response B: "It's like finding a name in a phone book..."

Human rates: A > B (A is more technical and complete)
```

**Step 3: Train Reward Model**
```
R(prompt, Response A) = 0.85
R(prompt, Response B) = 0.45
```

**Step 4: RL Fine-tuning**
```python
for batch in dataset:
    # Generate response using current policy
    response = LLM.generate(prompt, sample=True)
    
    # Get reward from reward model
    reward = RewardModel(prompt, response)
    
    # PPO update: Increase probability of high-reward responses
    advantage = reward - baseline_value
    loss = -log_prob(response) √ó advantage + KL_penalty
    
    LLM.update(loss)
```

This is conceptually identical to Q-learning's TD update, just applied to text generation!

**Summary:**

The RL principles from Ms. Pac-Man‚Äîexploration-exploitation, temporal credit assignment, value functions, policy optimization, and reward shaping‚Äîdirectly power modern LLM training. Understanding DQN provides insight into how frontier AI systems learn to be helpful, harmless, and honest through reinforcement learning.

---

### **5. Planning in RL vs LLM Agents**

**How does planning differ between traditional RL and LLM-based agents?**

**Planning Horizon:**

**DQN (Implicit, Fixed Horizon):**
- Planning horizon determined by discount factor Œ≥
- Effective horizon: -ln(0.01) / ln(Œ≥) ‚âà 460 steps for Œ≥=0.99
- **Fixed:** Cannot dynamically adjust planning depth
- **Implicit:** Encoded in Q-values, not explicit reasoning

**LLM (Explicit, Variable Horizon):**
- Can plan 3 steps or 1000 steps based on task
- **Variable:** Adjust planning depth as needed
- **Explicit:** Plans stated in natural language

**Planning Mechanism:**

**DQN: Reactive Value-Based**
```python
# No explicit plan‚Äîjust value lookup
state = current_pixels
q_values = network(state)
action = argmax(q_values)
# Decision made in ~1ms
```

**LLM: Deliberative Symbolic**
```python
plan = llm.generate("""
State: Ms. Pac-Man near power pellet, ghost approaching
Create a 10-step plan to maximize score.
""")
# Output:
# "Step 1: Move right to power pellet (5 moves)
#  Step 2: Eat power pellet
#  Step 3: Turn and chase red ghost
#  Step 4: Capture red ghost (+200)
#  Step 5: Chase pink ghost
#  Step 6: Capture pink ghost (+400)
#  Step 7: Retreat to safe corner
#  Step 8: Resume pellet collection"
# Decision takes ~2 seconds
```

**Concrete Example: Navigation Decision**

**Scenario:** Ms. Pac-Man at intersection, ghost nearby

**DQN Planning:**
```
Input: [84√ó84√ó4 pixel array]
Process: CNN forward pass (3ms)
Output: Q(UP)=2.1, Q(RIGHT)=3.8, Q(LEFT)=1.4, Q(DOWN)=0.9
Decision: Move RIGHT (highest Q-value)

No explicit reasoning about:
- Why right is better
- What will happen next
- Long-term consequences
```

**LLM Planning:**
```
Input: "Ms. Pac-Man at (20,30). Ghost at (18,28) moving right. 
        Power pellet at (25,35). Pellets in right corridor."

Reasoning: 
"Analysis of options:
 - LEFT: Moves toward ghost (dangerous, -500 expected)
 - RIGHT: Leads to power pellet corridor (safe, +200 expected)
 - UP/DOWN: Neutral positioning
 
 Decision: Move RIGHT
 Rationale: 
  1. Increases distance from approaching ghost
  2. Positions for power pellet access
  3. Right corridor has uncollected pellets
  4. Strategic setup for ghost-hunting sequence
 
 Expected value: +200 points over next 15 steps"

Decision: Move RIGHT (with full justification)
```

**Framework Comparison:**

**DQN: Markov Decision Process**
```
Components:
- States (S): Pixel observations
- Actions (A): 9 discrete movements
- Transitions: P(s'|s,a) - mostly deterministic
- Rewards: R(s,a) - from game engine
- Value: V(s), Q(s,a) - learned functions

Planning: Dynamic programming (Bellman backups)
```

**LLM: Symbolic Reasoning**
```
Components:
- World model: Learned from text (not explicit probabilities)
- Common sense: General knowledge about game mechanics
- Causal reasoning: If-then logical inference
- Goals: Natural language objectives

Planning: Chain-of-thought heuristic search
```

**Real-World Application: Robot Task Planning**

**Task:** "Make a cup of coffee"

**DQN Approach:**
- Train on millions of coffee-making simulations
- Learn: State (cup position, water level) ‚Üí Action (pour, tilt)
- Reactive execution based on visual input
- Fast, precise motor control
- Cannot adapt to "make tea" without retraining

**LLM Approach:**
- Read coffee-making instructions once
- Generate plan: "1. Grind beans, 2. Boil water, 3. Pour water over grounds, 4. Wait 4 min, 5. Add milk"
- Can adapt: "Make tea instead" ‚Üí generates new plan
- Slower but more flexible

**Hybrid Architecture:**
```python
class CoffeeMaker:
    def make_beverage(self, request):
        # LLM: High-level task planning
        plan = llm.generate(f"How to make {request}?")
        # "Steps: 1. Grind beans, 2. Heat water to 200¬∞F, 3. Pour slowly..."
        
        # DQN: Low-level motor control
        for step in plan:
            action_sequence = dqn.execute(step, sensor_data)
            # Precise: grasping force, pouring angle, timing
        
        return beverage
```

**Key Insight:**

Planning in DQN is **implicit** (encoded in Q-values), **reactive** (immediate action selection), and **fast** (milliseconds).

Planning in LLMs is **explicit** (natural language), **deliberative** (multi-step reasoning), and **slow** (seconds).

Neither is universally superior‚Äîthey excel in different contexts and are increasingly combined in hybrid systems.

---

### **6. LLM Integration with DQN**

**How could you integrate a DQN agent with an LLM-based system?**

Several architectural patterns enable powerful hybrid systems:

**Architecture 1: Hierarchical LLM-DQN**

```python
class HierarchicalGameAgent:
    """LLM provides strategy, DQN executes tactics"""
    
    def __init__(self):
        self.llm = LanguageModel()
        self.dqn = DQNAgent()
        self.strategy_update_freq = 100  # Replan every 100 steps
    
    def play_episode(self):
        state = env.reset()
        step = 0
        
        # LLM generates initial strategic plan
        strategy = self.llm.plan(f"""
        Ms. Pac-Man game start. Create a strategic plan:
        1. Which quadrant to clear first?
        2. When to use power pellets?
        3. How to maximize ghost-eating opportunities?
        """)
        # Output: "Phase 1: Clear top-left (safe), Phase 2: Get power pellet,
        #          Phase 3: Hunt ghosts in bottom-right, Phase 4: Final sweep"
        
        while not done:
            # DQN executes within current strategic phase
            current_phase = self.parse_phase(strategy, step)
            action = self.dqn.act(state, constraints=current_phase)
            
            state, reward, done = env.step(action)
            step += 1
            
            # Periodically re-plan based on new information
            if step % self.strategy_update_freq == 0:
                progress = self.evaluate_progress(state)
                strategy = self.llm.update_plan(state, progress, strategy)
```

**Benefits:**
- LLM handles long-term planning (100-500 step horizons)
- DQN handles precise moment-to-moment control
- Best of both: Strategic intelligence + tactical precision

**Architecture 2: LLM as Reward Shaper**

Provide dense reward signals to accelerate DQN learning:

```python
def compute_reward(state, action, next_state):
    # Sparse environment reward
    env_reward = environment.score()
    
    # Dense LLM feedback every step
    llm_score = llm.evaluate(f"""
    Rate this action (0-10):
    - Safety: {state_description}
    - Progress toward goal: {action_description}
    - Strategic value: {outcome_description}
    """)
    
    # Combined reward
    total_reward = env_reward + 0.1 * llm_score
    return total_reward
```

**Architecture 3: LLM for State Abstraction**

Convert raw pixels to semantic features:

```python
# Instead of learning from pixels
state_pixels = [84√ó84√ó4 array]

# LLM extracts semantic state
semantic_state = llm.analyze(state_pixels, """
Extract:
- Player position (x, y)
- Ghost positions and states
- Pellet locations
- Threat level (0-10)
- Opportunity score (0-10)
""")
# Output: {player: (42,78), ghosts: [...], threat: 7, opportunity: 3}

# DQN learns from compact semantic features
q_values = dqn.network(semantic_features)  # 50 dimensions vs 28,224
```

**Real-World Applications:**

**1. Game NPCs:**
- LLM: "Player is aggressive ‚Üí switch to defensive strategy"
- DQN: Executes combat moves, dodging, attack timing

**2. Robotics:**
- LLM: Interprets "make coffee" ‚Üí generates task plan
- DQN: Precise motor control for grasping, pouring

**3. Trading:**
- LLM: "Market analysis: tech sector weak ‚Üí reduce exposure"
- DQN: Optimal order placement, timing, position sizing

**4. Autonomous Vehicles:**
- LLM: Route planning, strategic navigation
- DQN: Lane keeping, speed control, obstacle avoidance

---

## üìÅ Repository Structure

```
ms-pacman-dqn/
‚îÇ
‚îú‚îÄ‚îÄ src/                          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ train.py                  # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ eval_agent.py             # Evaluation script
‚îÇ   ‚îú‚îÄ‚îÄ model.py                  # DQN network architecture
‚îÇ   ‚îú‚îÄ‚îÄ replay.py                 # Experience replay buffer
‚îÇ   ‚îú‚îÄ‚îÄ wrappers.py               # Environment preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ policies.py               # Exploration policies
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                 # Training logging
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                  # Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ plot_training.py          # Visualization script
‚îÇ   ‚îú‚îÄ‚îÄ smoke_test.py             # Environment sanity check
‚îÇ   ‚îî‚îÄ‚îÄ preproc_smoke.py          # Preprocessing test
‚îÇ
‚îú‚îÄ‚îÄ configs/                      # Experiment configurations
‚îÇ   ‚îú‚îÄ‚îÄ baseline.yaml             # Baseline configuration
‚îÇ   ‚îú‚îÄ‚îÄ epsfast.yaml              # Fast epsilon decay
‚îÇ   ‚îú‚îÄ‚îÄ epsfloor.yaml             # Higher epsilon floor
‚îÇ   ‚îú‚îÄ‚îÄ gamma95.yaml              # Lower discount factor
‚îÇ   ‚îú‚îÄ‚îÄ lr0005.yaml               # Higher learning rate
‚îÇ   ‚îî‚îÄ‚îÄ softmax.yaml              # Softmax exploration
‚îÇ
‚îú‚îÄ‚îÄ docs/                         # Documentation and images
‚îÇ   ‚îú‚îÄ‚îÄ images/                   # Gameplay screenshots
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gameplay_frame_50.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gameplay_frame_150.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gameplay_frame_300.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gameplay_frame_420.png
‚îÇ   ‚îî‚îÄ‚îÄ report.pdf                # Full assignment report
‚îÇ
‚îú‚îÄ‚îÄ runs/                         # Training outputs (gitignored)
‚îÇ   ‚îî‚îÄ‚îÄ [run_name-timestamp]/
‚îÇ       ‚îú‚îÄ‚îÄ config.json
‚îÇ       ‚îú‚îÄ‚îÄ train_log.csv
‚îÇ       ‚îú‚îÄ‚îÄ eval_summary.json
‚îÇ       ‚îú‚îÄ‚îÄ best.pt               # Best model checkpoint
‚îÇ       ‚îú‚îÄ‚îÄ final.pt              # Final model checkpoint
‚îÇ       ‚îú‚îÄ‚îÄ plot_returns.png
‚îÇ       ‚îú‚îÄ‚îÄ plot_lengths.png
‚îÇ       ‚îî‚îÄ‚îÄ plot_loss.png
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ README.md                     # This file
‚îú‚îÄ‚îÄ LICENSE                       # MIT License
‚îî‚îÄ‚îÄ .gitignore                    # Git ignore rules
```

---

## üî¨ Code Attribution

### **Original Implementation (40%)**

The following components were **written from scratch** by me:

1. **Training Orchestration** (`train.py`, lines 1-140)
   - Complete training loop with episode management
   - Frame-by-frame interaction with environment
   - Checkpoint saving and logging integration

2. **CSV Logging System** (`logger.py`, lines 1-45)
   - Custom TrainLog dataclass
   - CSV writing with episode metrics
   - Best model tracking based on loss

3. **Configuration System** (`configs/*.yaml`, 6 files)
   - YAML-based hyperparameter management
   - All 6 experiment configurations

4. **Evaluation Pipeline** (`eval_agent.py`, lines 1-80)
   - Greedy policy evaluation (Œµ=0)
   - Statistics calculation and JSON export

5. **Visualization** (`plot_training.py`, lines 1-40)
   - Matplotlib-based training curve generation
   - Automatic plot saving

### **Adapted from External Sources (60%)**

The following components were **adapted and modified** from established implementations:

**1. DQN Network Architecture** (`model.py`, lines 1-35)
- **Original Source:** Mnih et al. (2015), "Human-level control through deep reinforcement learning", *Nature* 518:529-533
- **URL:** https://www.nature.com/articles/nature14236
- **Original Design:** 3 convolutional layers (8√ó8, 4√ó4, 3√ó3) + 2 fully connected layers
- **My Modifications:**
  - Added explicit pixel normalization (divide by 255.0 in forward pass)
  - Implemented Kaiming uniform initialization for ReLU networks
  - Adjusted output layer for Ms. Pac-Man's 9 actions
  - Restructured as PyTorch nn.Module with clear documentation

**2. Experience Replay Buffer** (`replay.py`, lines 1-40)
- **Original Source:** OpenAI Spinning Up documentation
- **URL:** https://spinningup.openai.com/en/latest/algorithms/dqn.html
- **My Modifications:**
  - Numpy-based implementation instead of Python deque
  - Separate arrays for each component (obs, actions, rewards, next_obs, dones)
  - uint8 dtype for observation storage (memory efficiency)
  - Custom sampling method returning numpy arrays

**3. Preprocessing Wrappers** (`wrappers.py`, lines 1-60)
- **Original Source:** Farama Gymnasium documentation and examples
- **URL:** https://gymnasium.farama.org/
- **My Modifications:**
  - Custom PreprocessObs using OpenCV (cv2) for efficient resizing
  - FrameStack using collections.deque for memory efficiency
  - ClipReward wrapper for reward normalization
  - Type hints and documentation added

**4. Exploration Policies** (`policies.py`, lines 1-20)
- **Original Source:** Sutton & Barto (2018), *Reinforcement Learning: An Introduction*
- **My Modifications:**
  - Implemented both Œµ-greedy and softmax (Boltzmann) exploration
  - Numerically stable softmax with temperature parameter
  - Temperature scheduling integrated with epsilon decay

**5. Utility Functions** (`utils.py`, lines 1-15)
- **Original Source:** Standard PyTorch/RL patterns
- **My Implementation:**
  - Custom device selection (MPS for Apple Silicon, CUDA, or CPU)
  - Seed setting across all libraries (random, numpy, torch)
  - YAML configuration loading

### **External Libraries**

All external libraries are properly licensed and cited:

```python
import gymnasium as gym         # Farama Foundation - MIT License
import ale_py                   # Arcade Learning Environment - GPL 2.0
import torch                    # Meta/PyTorch - BSD License
import numpy as np             # NumPy Developers - BSD License
import cv2                     # OpenCV - Apache 2.0 License
import matplotlib.pyplot as plt # Matplotlib - PSF License
import yaml                    # PyYAML - MIT License
```

### **Code Percentage Breakdown**

- **Original code:** ~40% (training loops, logging, configs, eval, plotting)
- **Adapted code:** ~50% (DQN architecture, replay buffer, wrappers, policies)
- **External libraries:** ~10% (standard usage of PyTorch, Gymnasium, NumPy)

### **Academic Integrity Statement**

I certify that:
1. All adapted code is properly attributed with sources and URLs
2. I have made substantial modifications to all adapted code
3. I understand all code in this project and can explain any part
4. This work represents my learning and implementation effort

**Signature:** Sri Lakshmi Swetha Jalluri  
**Date:** November 2025

---

## üìú License

### **MIT License**

```
MIT License

Copyright (c) 2025 Sri Lakshmi Swetha Jalluri

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

**Why MIT License?**
- ‚úÖ Permissive: Allows educational and commercial use
- ‚úÖ Compatible with all dependencies (PyTorch BSD, Gymnasium MIT)
- ‚úÖ Industry standard for academic projects
- ‚úÖ Simple and well-understood terms

---

## üé• Video Demonstration

**üìπ Watch Full Demo:** [https://youtu.be/YOUR_VIDEO_ID_HERE](https://youtu.be/YOUR_VIDEO_ID_HERE)

**Video Contents:**
1. **Introduction** (1 min) - Project overview and objectives
2. **Code Walkthrough** (5 min) - DQN architecture, training loop, key components
3. **Training Analysis** (2 min) - Experimental results and comparisons
4. **Live Gameplay** (2 min) - Trained agent playing Ms. Pac-Man with narration
5. **Conclusion** (1 min) - Key findings and future work

**Duration:** 10-12 minutes

---

## üìö References

1. Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529-533. https://doi.org/10.1038/nature14236

2. Van Hasselt, H., Guez, A., & Silver, D. (2016). Deep Reinforcement Learning with Double Q-learning. *Proceedings of the AAAI Conference on Artificial Intelligence*.

3. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.

4. Bellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2013). The arcade learning environment: An evaluation platform for general agents. *Journal of Artificial Intelligence Research*, 47, 253-279.

5. Farama Foundation. (2024). Gymnasium Documentation. https://gymnasium.farama.org/

6. OpenAI. (2018). Spinning Up in Deep RL. https://spinningup.openai.com/

7. Ouyang, L., Wu, J., Jiang, X., et al. (2022). Training language models to follow instructions with human feedback. *arXiv preprint arXiv:2203.02155*.

---

## üôè Acknowledgments

- **Farama Foundation** for Gymnasium and ALE environments
- **OpenAI** for DQN implementation guidance and documentation
- **PyTorch Team** for the deep learning framework
- **[Your Professor/TA Names]** for course instruction and feedback

---

## üìû Contact

**Sri Lakshmi Swetha Jalluri**  
üìß Email: [your.email@university.edu]  
üîó GitHub: [@yourusername](https://github.com/yourusername)  
üíº LinkedIn: [your-linkedin](https://linkedin.com/in/your-profile)

---

## üéì Academic Use

This project was completed as coursework for [Course Name] at [University Name].

**For Academic Reviewers:**
- All external sources are properly attributed
- Code implements DQN from first principles with referenced guidance
- Experiments were conducted independently
- Results are reproducible with provided seeds

**For Future Students:**
- You may reference this work for learning
- You must NOT submit this code as your own work
- Cite this project if it informs your understanding
- Follow your institution's academic integrity policies

---

## üöß Known Issues & Future Work

### **Current Limitations**
- Training limited to 300k-500k frames (could extend to 2M+ for better convergence)
- Vanilla DQN (not Double DQN, Dueling DQN, or Rainbow)
- No prioritized experience replay
- Evaluation on single seed only

### **Planned Improvements**
- [ ] Implement Double DQN to address overestimation bias
- [ ] Add Dueling DQN architecture for better value estimation
- [ ] Integrate prioritized experience replay
- [ ] Multi-seed evaluation for statistical robustness
- [ ] Extend training to 2M frames for full convergence
- [ ] Implement curiosity-driven exploration
- [ ] Add TensorBoard logging for real-time monitoring

---

## ‚ö° Quick Start Guide

```bash
# 1. Clone and setup
git clone https://github.com/yourusername/dqn-mspacman.git
cd dqn-mspacman
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. Verify installation
python src/smoke_test.py

# 3. Train baseline agent (quick 20k frame test)
python src/train_sanity.py

# 4. Train full agent (300k frames, ~4-6 hours)
python src/train.py --config configs/lr0005.yaml

# 5. Evaluate trained agent
python src/eval_agent.py --run_dir runs/lr0005-300k-[timestamp] --episodes 10

# 6. Generate plots
python src/plot_training.py runs/lr0005-300k-[timestamp]
```

---

<div align="center">

**‚≠ê If this project helped you learn DQN, please consider starring the repository! ‚≠ê**

Made with üß† and ‚òï by Sri Lakshmi Swetha Jalluri

[Report Issues](https://github.com/yourusername/dqn-mspacman/issues) ‚Ä¢ [Request Features](https://github.com/yourusername/dqn-mspacman/issues/new)

</div>
